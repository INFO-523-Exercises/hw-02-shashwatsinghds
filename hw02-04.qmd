---
title: "hw02-04"
author: "Shashwat Singh"
format: html
editor: visual
description: "Imputing like a Data Scientist: Exploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set and produce publication quality graphs and tables."
---

# **Imputing like a Data Scientist**

# Install Required Packages and Environment Setup

```{r, warning= FALSE}
# Sets the number of significant figures to two - e.g., 0.01
options(digits = 2)

# Required package for quick package downloading and loading 
if (!require(pacman))
  install.packages("pacman")

pacman::p_load(colorblindr, # Colorblind friendly pallettes
               cluster, # K cluster analyses
               dlookr, # Exploratory data analysis
               formattable, # HTML tables from R outputs
               ggfortify, # Plotting tools for stats
               ggpubr, # Publishable ggplots
               here, # Standardizes paths to data
               kableExtra, # Alternative to formattable
               knitr, # Needed to write HTML reports
               missRanger, # To generate NAs
               plotly, # Visualization package
               rattle, # Decision tree visualization
               rpart, # rpart algorithm
               tidyverse, # Powerful data wrangling package suite
               visdat) # Another EDA visualization package

# I was having issues with installing colorblindr so I skipped it
# Set global ggplot() theme
# Theme pub_clean() from the ggpubr package with base text size = 16
theme_set(theme_pubclean(base_size = 16)) 
# All axes titles to their respective far right sides
theme_update(axis.title = element_text(hjust = 1))
# Remove axes ticks
theme_update(axis.ticks = element_blank()) 
# Remove legend key
theme_update(legend.key = element_blank())
```

# Load and Examine a Data Set

-   Load data and view
-   Examine columns and data types
-   Define box plots
-   Describe meta data

Using the dataset from tidyurl- The dataset contains stock prices of big tech companies and tries to analyse the collapse of "Big Tech" stock prices. The data comes from Yahoo Finance via Kaggle

```{r}
# Loading the dataset
dataset <- read.csv("data/big_tech_stock_prices.csv") |>
  mutate(
    OpenGroup = ifelse(open >= 1 & open <= 10, "LowOpen", 
                       ifelse(open > 10 & open <= 100, "MediumOpen", "HighOpen"))
  )

# Formattable data
dataset |>
  head() |>
  formattable()
```

# Diagnose your Data

```{r}
#Properties of the data
dataset |>
  diagnose() |>
  formattable()
```

-   `variables`: name of each variable

-   `types`: data type of each variable

-   `missing_count`: number of missing values

-   `missing_percent`: percentage of missing values

-   `unique_count`: number of unique values

-   `unique_rate`: rate of unique value - unique_count / number of observations

# **Diagnose Outliers**

There are several numerical variables that have outliers above, let's see what the data look like with and without them

-   Create a table with columns containing outliers

-   Plot outliers in a box plot and histogram

```{r, warning=FALSE}
# Table showing outliers
dataset |>
  diagnose_outlier() |>
  filter(outliers_ratio > 0) |>  
  mutate(rate = outliers_mean / with_mean) |>
  arrange(desc(rate)) |> 
  select(-outliers_cnt) |>
  formattable()
```

```{r}
# Boxplots and histograms of data with and without outliers
dataset |>
  select(find_outliers(dataset)) |>
           plot_outlier()
```

Plots for all variables are shifting towards normality without outliers.

# **Basic Exploration of Missing Values (NAs)**

-   Table showing the extent of NAs in columns containing them

```{r}
# Randomly generate NAs for 30
na.dataset <- dataset |>
  generateNA(p = 0.3)

# First six rows
na.dataset |>
head() |>
  formattable()
```

```{r}
# Create the NA table
na.dataset |>
  plot_na_pareto(only_na = TRUE, plot = FALSE) |>
  formattable() # Publishable table
```

-   Plots showing the frequency of missing values

```{r}
# Plot the intersect of the columns with missing values
# This plot visualizes the table above
na.dataset |>
  plot_na_pareto(only_na = TRUE)
```

# **Advanced Exploration of Missing Values (NAs)**

-   Intersect plot that shows, for every combination of columns relevant, how many missing values are common

-   Orange boxes are the columns in question

-   x axis (top green bar plots) show the number of missing values in that column

-   y axis (right green bars) show the number of missing values in the columns in orange blocks

    ```{r}
    # Plot the intersect of the 5 columns with the most missing values
    # This means that some combinations of columns have missing values in the same row
    na.dataset |>
      select(high, low, close) |>
      plot_na_intersect(only_na = TRUE) 
    ```

## **Determining if NA Observations are the Same**

-   Missing values can be the same observation across several columns, this is not shown above

-   The visdat package can solve this with the `vis_miss()` function which shows the rows with missing values through `ggplotly()`

-   Here we will show ALL columns with NAs, and you can zoom into individual rows (interactive plot)

-   NOTE: This line will make the HTML rendering take a while...

    ```{r, eval=FALSE}
    # Interactive plotly() plot of all NA values to examine every row
    na.dataset |>
     select(high, low, close) |>
     vis_miss() |>
     ggplotly() 

    # This is taking too much time and then failing, so I have set eval=FALSE
    ```

# **Impute Outliers and NAs**

Removing outliers and NAs can be tricky, but there are methods to do so. I will go over several, and discuss benefits and costs to each.

The principle goal for all imputation is to find the method that does not change the distribution too much (or oddly).

## Classifying Outliers

Before imputing outliers, you will want to diagnose whether it's they are natural outliers or not. We will be looking at "Close" for example across OpenGroup(mutated earlier), because there are outliers and several NAs, which we will impute below.

```{r}
# Box plot
dataset %>% # Set the simulated normal data as a data frame
  ggplot(aes(x = close, y = OpenGroup, fill = OpenGroup)) + # Create a ggplot
  geom_boxplot(width = 0.5, outlier.size = 2, outlier.alpha = 0.5) +
  xlab("Closing Price") +  # Relabel the x axis label
  ylab("OpenGroup") + # Remove the y axis label
  theme(legend.position = "none")  # Remove the legend
```

We remove outliers using imputate_outlier() and replace them with values that are estimates based on the existing data

mean: arithmetic mean

median: median

mode: mode

capping: Impute the upper outliers with 95 percentile, and impute the bottom outliers with 5 percentile - aka Winsorizing

## Mean Imputation

The mean of the observed values for each variable is computed and the outliers for that variable are imputed by this mean

```{r}
# Raw summary, output suppressed
mean_out_imp_close <- dataset |>
  select(close) |>
  filter(close > 100) |>
  imputate_outlier(close, method = "mean")

# Output showing the summary statistics of our imputation
mean_out_imp_close |>
  summary() 
```

```{r}
# Visualization of the mean imputation
mean_out_imp_close |>
  plot()
```

## Median Imputation

The median of the observed values for each variable is computed and the outliers for that variable are imputed by this median

```{r}
# Raw summary, output suppressed
median_out_imp_close <- dataset |>
  select(close) |>
  filter(close > 100) |>
  imputate_outlier(close, method = "median")

# Output showing the summary statistics of our imputation
median_out_imp_close |>
  summary() 
```

```{r}
# Visualization of the mean imputation
median_out_imp_close |>
  plot()
```

#### Pros & Cons of Using the Mean or Median Imputation

**Pros**:

-   Easy and fast.

-   Works well with small numerical datasets.

**Cons**:

-   Doesn't factor the correlations between variables. It only works on the column level.

-   Will give poor results on encoded categorical variables (do **NOT** use it on categorical variables).

-   Not very accurate.

-   Doesn't account for the uncertainty in the imputations.

## Mode Imputation

The mode of the observed values for each variable is computed and the outliers for that variable are imputed by this mode

```{r}
# Raw summary, output suppressed
mode_out_imp_close <- dataset |>
  select(close) |>
  filter(close > 100) |>
  imputate_outlier(close, method = "mode")

# Output showing the summary statistics of our imputation
mode_out_imp_close |>
  summary() 
```

```{r}
# Visualization of the mean imputation
mode_out_imp_close |>
  plot()
```

#### Pros & Cons of Using the Mode Imputation

**Pros**:

-   Works well with categorical variables.

**Cons**:

-   It also doesn't factor the correlations between variables.

-   It can introduce bias in the data.

## **Capping Imputation (aka Winsorizing)**

The Percentile Capping is a method of Imputing the outlier values by replacing those observations outside the lower limit with the value of 5th percentile and those that lie above the upper limit, with the value of 95th percentile of the same dataset.

```{r}
# Raw summary, output suppressed
cap_out_imp_close <- dataset |>
  select(close) |>
  filter(close > 100) |>
  imputate_outlier(close, method = "capping")

# Output showing the summary statistics of our imputation
cap_out_imp_close |>
  summary() 
```

```{r}
# Visualization of the mean imputation
cap_out_imp_close |>
  plot()
```

#### Pros and Cons of Capping

**Pros**:

-   Not influenced by extreme values

**Cons**:

-   Capping only modifies the smallest and largest values slightly. This is generally not a good idea since it means we're just modifying data values for the sake of modifications.

-   If no extreme outliers are present, Winsorization may be unnecessary.

## **Imputing NAs**

1.  `knn`: K-nearest neighbors (KNN)

2.  `rpart`: Recursive Partitioning and Regression Trees (rpart)

3.  `mice`: Multivariate Imputation by Chained Equations (MICE)

Since our normal `dataset` has no NA values, we will use the `na.dataset` we created earlier.

### **K-Nearest Neighbor (KNN) Imputation**

KNN is a machine learning algorithm that classifies data by similarity. This in effect clusters data into similar groups. The algorithm predicts values of new data to replace NA values based on how closely they resembles training data points, such as by comparing across other columns.

Here's a visual example using the `clara()` function from the `cluster` package to run a KNN algorithm on our `dataset`, where three clusters are created by the algorithm.

```{r}
# KNN plot of our dataset without categories
autoplot(clara(dataset[-5], 3)) 
```
Note that PC1 has 100% 

```{r, eval=FALSE}
# Raw summary, output suppressed
knn_na_imp_close <- na.dataset |>
  imputate_na(close, method = "knn")

# Plot showing the results of our imputation
knn_na_imp_close |>
  plot()


```
knn imputation does not work here.

#### Pros & Cons of Using KNN Imputation

**Pro**:

-   Possibly much more accurate than mean, median, or mode imputation for some data sets.

**Cons**:

-   KNN is computationally expensive because it stores the entire training dataset into computer memory.

-   KNN is very sensitive to outliers, so you would have to imputate these first.

### **Recursive Partitioning and Regression Trees (rpart)**

rpart is a decision tree machine learning algorithm that builds classification or regression models through a two stage process, which can be thought of as binary trees. The algorithm splits the data into subsets, which move down other branches of the tree until a termination criteria is reached.

For example, if we are missing a value for `OpenGroup` a first decision could be whether the associated `Close` is within a series of yes or no criteria

```{r, eval=FALSE}
# Raw summary, output suppressed
rpart_na_imp_close <- na.dataset |>
  imputate_na(close, method = "rpart")

# Plot showing the results of our imputation
rpart_na_imp_insulin |>
  plot()

# I am not able to use this imputation as well. I am unsure if it's because of the type of dataset that I have, as discussed in class.
```

#### Pros & Cons of Using rpart Imputation

**Pros**:

-   Good for categorical data because approximations are easier to compare across categories than continuous variables.

-   Not sensitive to outliers.

**Cons**:

-   Can over fit the data as they grow.

-   Speed decreases with more data columns.

### **Multivariate Imputation by Chained Equations (MICE)** 

MICE is an algorithm that fills missing values multiple times, hence dealing with uncertainty better than other methods. This approach creates multiple copies of the data that can then be analyzed and then pooled into a single dataset.

NOTE: You will have to set a random seed (e.g., 123) since the MICE algorithm pools several simulated imputations. Without setting a seed, a different result will occur after each simulation.

```{r, eval= FALSE}

library(mice)
mice_na_imp_close <- na.dataset |>
  imputate_na(close, method = "mice", seed = 29)

# R studio is not loading the mice library
```

```{r, eval=FALSE}
# Plot showing the results of our imputation
mice_na_imp_insulin |>
  plot()
```

#### Pros & Cons of MICE Imputation

**Pros**:

-   Multiple imputations are more accurate than a single imputation.

-   The chained equations are very flexible to data types, such as categorical and ordinal.

**Cons**:

-   You have to round the results for ordinal data because resulting data points are too great or too small (floating-points).

# Produce an HTML Transformation Summary

```{r, eval= FALSE}
transformation_web_report(dataset)
```

